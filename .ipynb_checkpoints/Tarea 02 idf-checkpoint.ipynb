{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N921vKRx4RIH"
   },
   "source": [
    "# Tarea 02: idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd_x9Fv4rMre"
   },
   "source": [
    "### En esta tarea, se trabajará con un corpus de textos para obtener las palabras mas y menos relevantes a partir de un análisis por tf-idf. Deberás seguir los pasos de este notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nPaso 1) \\nDel archivo adjunto (Questions.csv), extraer las primeras 2,000 preguntas presentes en la columna \"Title\", \\ny generar un nuevo archivo que SOLO contenga dichas preguntas.\\nNOTA: El archivo es muy grande, por lo que deberás de procesarlo en tu equipo local (No usar colab), y \\ngenerar con ayuda de un script el nuevo archivo, si llegas a tener problemas con el tipo de codificación\\ny se generan caracteres raros, deberás resolverlo con la codificación adecuada de lectura\\n\\nPaso 2) \\nTraducirlas las preguntas del nuevo archivo al idioma Español (Investiga cómo se realiza el proceso de \\ntraducción automático utilizando Google Sheets y Google Translate)\\nSe deberá de agregar una nueva columna al archivo .csv llamada \"Textos_traducidos\" donde se incluirán las \\ntraducciones de los textos originales (Incluir este nuevo .csv en la entrega de tu tarea)\\n\\nPaso 3) \\nCon este nuevo .csv de 2,000 textos traducidos al español, crear un dataset (de 2,000 textos) y aplicar \\nOBLIGATORIAMENTE los siguientes preprocesamientos:\\n- Lematización de todas las palabras\\n- Filtrado de StopWords\\n- Pasar todo a minúsculas\\n\\nPaso 4) \\nDespués de haber limpiado el dataset anterior, generar el vector de idf correspondiente a TODOS los textos\\ny mostrarlo en pantalla\\n\\nPaso 5) Regresar al Dataset original, y remover todas aquellas palabras que contengan un valor de idf menor\\nal promedio de TODOS los idfs de la tabla obtenida\\n\\nPaso 6)\\nGenerar nuevamente la tabla de idf a partir de valores de idf para los textos filtrados y mostrarla en pantalla\\n\\nPaso 7) Imprimir en pantalla el top de las 10 palabras MAS relevantes, y el top de las 20 palabras MENOS \\nrelevantes\\n\\nNOTA: Recuerda que deberás de entregar 2 archivos, un .csv con los 2,000 textos originales y sus traducciones\\ny un .ipynb con todo el procedimiento realizado con sus respectivos comentarios, y DocStrings\\nIMPORTANTE: Todo el proceso deberá realizarse por medio de métodos, NO se aceptará programación estructurada,\\npor lo que, por ejemplo, deberá haber un método para filtrar StopWords, otro para obtener el promedio de \\nidf de todo el conjunto de palabras, etc. \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Paso 1) \n",
    "Del archivo adjunto (Questions.csv), extraer las primeras 2,000 preguntas presentes en la columna \"Title\", \n",
    "y generar un nuevo archivo que SOLO contenga dichas preguntas.\n",
    "NOTA: El archivo es muy grande, por lo que deberás de procesarlo en tu equipo local (No usar colab), y \n",
    "generar con ayuda de un script el nuevo archivo, si llegas a tener problemas con el tipo de codificación\n",
    "y se generan caracteres raros, deberás resolverlo con la codificación adecuada de lectura\n",
    "\n",
    "Paso 2) \n",
    "Traducirlas las preguntas del nuevo archivo al idioma Español (Investiga cómo se realiza el proceso de \n",
    "traducción automático utilizando Google Sheets y Google Translate)\n",
    "Se deberá de agregar una nueva columna al archivo .csv llamada \"Textos_traducidos\" donde se incluirán las \n",
    "traducciones de los textos originales (Incluir este nuevo .csv en la entrega de tu tarea)\n",
    "\n",
    "Paso 3) \n",
    "Con este nuevo .csv de 2,000 textos traducidos al español, crear un dataset (de 2,000 textos) y aplicar \n",
    "OBLIGATORIAMENTE los siguientes preprocesamientos:\n",
    "- Lematización de todas las palabras\n",
    "- Filtrado de StopWords\n",
    "- Pasar todo a minúsculas\n",
    "\n",
    "Paso 4) \n",
    "Después de haber limpiado el dataset anterior, generar el vector de idf correspondiente a TODOS los textos\n",
    "y mostrarlo en pantalla\n",
    "\n",
    "Paso 5) Regresar al Dataset original, y remover todas aquellas palabras que contengan un valor de idf menor\n",
    "al promedio de TODOS los idfs de la tabla obtenida\n",
    "\n",
    "Paso 6)\n",
    "Generar nuevamente la tabla de idf a partir de valores de idf para los textos filtrados y mostrarla en pantalla\n",
    "\n",
    "Paso 7) Imprimir en pantalla el top de las 10 palabras MAS relevantes, y el top de las 20 palabras MENOS \n",
    "relevantes\n",
    "\n",
    "NOTA: Recuerda que deberás de entregar 2 archivos, un .csv con los 2,000 textos originales y sus traducciones\n",
    "y un .ipynb con todo el procedimiento realizado con sus respectivos comentarios, y DocStrings\n",
    "IMPORTANTE: Todo el proceso deberá realizarse por medio de métodos, NO se aceptará programación estructurada,\n",
    "por lo que, por ejemplo, deberá haber un método para filtrar StopWords, otro para obtener el promedio de \n",
    "idf de todo el conjunto de palabras, etc. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "7InFgho6rytt"
   },
   "outputs": [],
   "source": [
    "# Recuerda que todos los métodos que utilices deberán de contar con el formato DocString\n",
    "# como en el ejemplo que se muestra a continuación:\n",
    "# Ejemplo de formato Docstrings:\n",
    "\n",
    "def NombreFuncion(arg1, arg2, arg3):\n",
    "    \"\"\"\n",
    "    Este método sirve para... utilizando... y devuelve...\n",
    "    \n",
    "    Args:\n",
    "        string arg1: Esta es una cadena de texto que...\n",
    "        int arg 2: Es un número entero que se usa para...\n",
    "        dict arg 3: Diccionario que sirve para...\n",
    "\n",
    "    Returns:\n",
    "        string: Cadena del texto ya corregido...\n",
    "        int: El la cantidad de correcciones realizadas...\n",
    "    \"\"\"\n",
    "\n",
    "    # Aquí debe de ir la lógica de la función (Después de la documentación)\n",
    "    Texto = \"\"\n",
    "    corr = 5\n",
    "    \n",
    "    return Texto, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Paso 1) \n",
    "Del archivo adjunto (Questions.csv), extraer las primeras 2,000 preguntas presentes en la columna \"Title\", \n",
    "y generar un nuevo archivo que SOLO contenga dichas preguntas.\n",
    "NOTA: El archivo es muy grande, por lo que deberás de procesarlo en tu equipo local (No usar colab), y \n",
    "generar con ayuda de un script el nuevo archivo, si llegas a tener problemas con el tipo de codificación\n",
    "y se generan caracteres raros, deberás resolverlo con la codificación adecuada de lectura \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Se lee el archivo Questions.csv\n",
    "df = pd.read_csv(\"Questions.csv\", encoding='latin-1')\n",
    "\n",
    "# Se extrae la columna Title\n",
    "df = df[\"Title\"]\n",
    "\n",
    "# Se extraen las primeras 2000 preguntas \n",
    "df = df.head(2000)\n",
    "\n",
    "# Se crea un nuevo archivo csv con las preguntas extraidas\n",
    "df.to_csv(\"Preguntas.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'group'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mfor\u001b[39;00m pregunta \u001b[39min\u001b[39;00m df:\n\u001b[1;32m     23\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mtype\u001b[39m(pregunta))\n\u001b[0;32m---> 24\u001b[0m     traduccion \u001b[39m=\u001b[39m translator\u001b[39m.\u001b[39;49mtranslate(pregunta, src\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39men\u001b[39;49m\u001b[39m\"\u001b[39;49m, dest\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mes\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     25\u001b[0m     traducciones\u001b[39m.\u001b[39mappend(traduccion\u001b[39m.\u001b[39mtext)\n\u001b[1;32m     27\u001b[0m \u001b[39m# Se agrega la lista de traducciones al dataframe\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/googletrans/client.py:182\u001b[0m, in \u001b[0;36mTranslator.translate\u001b[0;34m(self, text, dest, src, **kwargs)\u001b[0m\n\u001b[1;32m    179\u001b[0m     \u001b[39mreturn\u001b[39;00m result\n\u001b[1;32m    181\u001b[0m origin \u001b[39m=\u001b[39m text\n\u001b[0;32m--> 182\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_translate(text, dest, src, kwargs)\n\u001b[1;32m    184\u001b[0m \u001b[39m# this code will be updated when the format is changed.\u001b[39;00m\n\u001b[1;32m    185\u001b[0m translated \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin([d[\u001b[39m0\u001b[39m] \u001b[39mif\u001b[39;00m d[\u001b[39m0\u001b[39m] \u001b[39melse\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m \u001b[39mfor\u001b[39;00m d \u001b[39min\u001b[39;00m data[\u001b[39m0\u001b[39m]])\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/googletrans/client.py:78\u001b[0m, in \u001b[0;36mTranslator._translate\u001b[0;34m(self, text, dest, src, override)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_translate\u001b[39m(\u001b[39mself\u001b[39m, text, dest, src, override):\n\u001b[0;32m---> 78\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtoken_acquirer\u001b[39m.\u001b[39;49mdo(text)\n\u001b[1;32m     79\u001b[0m     params \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mbuild_params(query\u001b[39m=\u001b[39mtext, src\u001b[39m=\u001b[39msrc, dest\u001b[39m=\u001b[39mdest,\n\u001b[1;32m     80\u001b[0m                                 token\u001b[39m=\u001b[39mtoken, override\u001b[39m=\u001b[39moverride)\n\u001b[1;32m     82\u001b[0m     url \u001b[39m=\u001b[39m urls\u001b[39m.\u001b[39mTRANSLATE\u001b[39m.\u001b[39mformat(host\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pick_service_url())\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/googletrans/gtoken.py:194\u001b[0m, in \u001b[0;36mTokenAcquirer.do\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdo\u001b[39m(\u001b[39mself\u001b[39m, text):\n\u001b[0;32m--> 194\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_update()\n\u001b[1;32m    195\u001b[0m     tk \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39macquire(text)\n\u001b[1;32m    196\u001b[0m     \u001b[39mreturn\u001b[39;00m tk\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/googletrans/gtoken.py:62\u001b[0m, in \u001b[0;36mTokenAcquirer._update\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m# this will be the same as python code after stripping out a reserved word 'var'\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m code \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mRE_TKK\u001b[39m.\u001b[39;49msearch(r\u001b[39m.\u001b[39;49mtext)\u001b[39m.\u001b[39;49mgroup(\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39mvar \u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     63\u001b[0m \u001b[39m# unescape special ascii characters such like a \\x3d(=)\u001b[39;00m\n\u001b[1;32m     64\u001b[0m code \u001b[39m=\u001b[39m code\u001b[39m.\u001b[39mencode()\u001b[39m.\u001b[39mdecode(\u001b[39m'\u001b[39m\u001b[39municode-escape\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'group'"
     ]
    }
   ],
   "source": [
    "\"\"\" Paso 2) \n",
    "Traducirlas las preguntas del nuevo archivo al idioma Español (Investiga cómo se realiza el proceso de \n",
    "traducción automático utilizando Google Sheets y Google Translate)\n",
    "Se deberá de agregar una nueva columna al archivo .csv llamada \"Textos_traducidos\" donde se incluirán las \n",
    "traducciones de los textos originales (Incluir este nuevo .csv en la entrega de tu tarea)\"\"\"\n",
    "# Se abre el archivo Preguntas.csv\n",
    "df = pd.read_csv(\"Preguntas.csv\", encoding='latin-1')\n",
    "\n",
    "# Se lee la columna Title\n",
    "df = df[\"Title\"]\n",
    "\n",
    "# Se importa la libreria de googletrans\n",
    "from googletrans import Translator\n",
    "\n",
    "# Se crea un objeto de la clase Translator\n",
    "translator = Translator()\n",
    "\n",
    "# Se crea una lista vacia para almacenar las traducciones\n",
    "traducciones = []\n",
    "\n",
    "# Se recorre la columna Title para traducir cada pregunta de ingles a español\n",
    "for pregunta in df:\n",
    "    print(type(pregunta))\n",
    "    traduccion = translator.translate(pregunta, src=\"en\", dest=\"es\")\n",
    "    traducciones.append(traduccion.text)\n",
    "\n",
    "# Se agrega la lista de traducciones al dataframe\n",
    "df[\"Textos_traducidos\"] = traducciones\n",
    "\n",
    "#Se agrega una nueva columana a preguntas.csv llamada Textos_traducidos\n",
    "df.to_csv(\"Preguntas.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Textos_traducidos'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3789\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3790\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3791\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Textos_traducidos'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mPreguntas.csv\u001b[39m\u001b[39m\"\u001b[39m, encoding\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mlatin-1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m \u001b[39m# Se lee la columna Textos_traducidos\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m df \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39;49m\u001b[39mTextos_traducidos\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     14\u001b[0m \u001b[39m# Se importa la libreria de nltk\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnltk\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3894\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3895\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3896\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3897\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3898\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pandas/core/indexes/base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3792\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(casted_key, \u001b[39mslice\u001b[39m) \u001b[39mor\u001b[39;00m (\n\u001b[1;32m   3793\u001b[0m         \u001b[39misinstance\u001b[39m(casted_key, abc\u001b[39m.\u001b[39mIterable)\n\u001b[1;32m   3794\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39many\u001b[39m(\u001b[39misinstance\u001b[39m(x, \u001b[39mslice\u001b[39m) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m casted_key)\n\u001b[1;32m   3795\u001b[0m     ):\n\u001b[1;32m   3796\u001b[0m         \u001b[39mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3797\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3798\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3799\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3800\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3801\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3802\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Textos_traducidos'"
     ]
    }
   ],
   "source": [
    "\"\"\" Paso 3) \n",
    "Con este nuevo .csv de 2,000 textos traducidos al español, crear un dataset (de 2,000 textos) y aplicar \n",
    "OBLIGATORIAMENTE los siguientes preprocesamientos:\n",
    "- Lematización de todas las palabras\n",
    "- Filtrado de StopWords\n",
    "- Pasar todo a minúsculas \"\"\"\n",
    "\n",
    "# Se abre el archivo Preguntas.csv\n",
    "df = pd.read_csv(\"Preguntas.csv\", encoding='latin-1')\n",
    "\n",
    "# Se lee la columna Textos_traducidos\n",
    "df = df[\"Textos_traducidos\"]\n",
    "\n",
    "# Se importa la libreria de nltk\n",
    "import nltk\n",
    "\n",
    "# Se descargan los paquetes de nltk\n",
    "nltk.download('punkt') #punkt es un tokenizador de oraciones \n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Lematiacion\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Se crea una lista vacia para almacenar las preguntas lematizadas\n",
    "preguntas_lematizadas = []\n",
    "\n",
    "# Se recorre la columna Textos_traducidos para lematizar cada pregunta\n",
    "for pregunta in df:\n",
    "    # Se tokeniza cada pregunta\n",
    "    tokens = nltk.word_tokenize(pregunta)\n",
    "    #Pasar a minusculas\n",
    "    tokens = [w.lower() for w in tokens]\n",
    "\n",
    "    # Se lematiza cada token\n",
    "    for token in tokens:\n",
    "        #Eliminar stopwords\n",
    "        if token not in nltk.corpus.stopwords.words('spanish'): #si el token no esta en stopwords se lematiza y se agrega a la lista de preguntas lematizadas\n",
    "            lematizado = lemmatizer.lemmatize(token)\n",
    "            # Se agrega el token lematizado a la lista de preguntas lematizadas\n",
    "            preguntas_lematizadas.append(lematizado)\n",
    "\n",
    "    # Se agrega la pregunta lematizada a la lista de preguntas lematizadas\n",
    "    preguntas_lematizadas.append(lematizado)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
