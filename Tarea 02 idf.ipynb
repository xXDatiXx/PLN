{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N921vKRx4RIH"
   },
   "source": [
    "# Tarea 02: idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nd_x9Fv4rMre"
   },
   "source": [
    "### En esta tarea, se trabajará con un corpus de textos para obtener las palabras mas y menos relevantes a partir de un análisis por tf-idf. Deberás seguir los pasos de este notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Paso 1) \n",
    "Del archivo adjunto (Questions.csv), extraer las primeras 2,000 preguntas presentes en la columna \"Title\", \n",
    "y generar un nuevo archivo que SOLO contenga dichas preguntas.\n",
    "NOTA: El archivo es muy grande, por lo que deberás de procesarlo en tu equipo local (No usar colab), y \n",
    "generar con ayuda de un script el nuevo archivo, si llegas a tener problemas con el tipo de codificación\n",
    "y se generan caracteres raros, deberás resolverlo con la codificación adecuada de lectura\n",
    "\n",
    "Paso 2) \n",
    "Traducirlas las preguntas del nuevo archivo al idioma Español (Investiga cómo se realiza el proceso de \n",
    "traducción automático utilizando Google Sheets y Google Translate)\n",
    "Se deberá de agregar una nueva columna al archivo .csv llamada \"Textos_traducidos\" donde se incluirán las \n",
    "traducciones de los textos originales (Incluir este nuevo .csv en la entrega de tu tarea)\n",
    "\n",
    "Paso 3) \n",
    "Con este nuevo .csv de 2,000 textos traducidos al español, crear un dataset (de 2,000 textos) y aplicar \n",
    "OBLIGATORIAMENTE los siguientes preprocesamientos:\n",
    "- Lematización de todas las palabras\n",
    "- Filtrado de StopWords\n",
    "- Pasar todo a minúsculas\n",
    "\n",
    "Paso 4) \n",
    "Después de haber limpiado el dataset anterior, generar el vector de idf correspondiente a TODOS los textos\n",
    "y mostrarlo en pantalla\n",
    "\n",
    "Paso 5) Regresar al Dataset original, y remover todas aquellas palabras que contengan un valor de idf menor\n",
    "al promedio de TODOS los idfs de la tabla obtenida\n",
    "\n",
    "Paso 6)\n",
    "Generar nuevamente la tabla de idf a partir de valores de idf para los textos filtrados y mostrarla en pantalla\n",
    "\n",
    "Paso 7) Imprimir en pantalla el top de las 10 palabras MAS relevantes, y el top de las 20 palabras MENOS \n",
    "relevantes\n",
    "\n",
    "NOTA: Recuerda que deberás de entregar 2 archivos, un .csv con los 2,000 textos originales y sus traducciones\n",
    "y un .ipynb con todo el procedimiento realizado con sus respectivos comentarios, y DocStrings\n",
    "IMPORTANTE: Todo el proceso deberá realizarse por medio de métodos, NO se aceptará programación estructurada,\n",
    "por lo que, por ejemplo, deberá haber un método para filtrar StopWords, otro para obtener el promedio de \n",
    "idf de todo el conjunto de palabras, etc. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7InFgho6rytt"
   },
   "outputs": [],
   "source": [
    "# Recuerda que todos los métodos que utilices deberán de contar con el formato DocString\n",
    "# como en el ejemplo que se muestra a continuación:\n",
    "# Ejemplo de formato Docstrings:\n",
    "\n",
    "def NombreFuncion(arg1, arg2, arg3):\n",
    "    \"\"\"\n",
    "    Este método sirve para... utilizando... y devuelve...\n",
    "    \n",
    "    Args:\n",
    "        string arg1: Esta es una cadena de texto que...\n",
    "        int arg 2: Es un número entero que se usa para...\n",
    "        dict arg 3: Diccionario que sirve para...\n",
    "\n",
    "    Returns:\n",
    "        string: Cadena del texto ya corregido...\n",
    "        int: El la cantidad de correcciones realizadas...\n",
    "    \"\"\"\n",
    "\n",
    "    # Aquí debe de ir la lógica de la función (Después de la documentación)\n",
    "    Texto = \"\"\n",
    "    corr = 5\n",
    "    \n",
    "  return Texto, corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd  # Importa la biblioteca Pandas para el manejo de datos en formato de DataFrame.\n",
    "import nltk  # Importa la biblioteca NLTK (Natural Language Toolkit) para el procesamiento de lenguaje natural.\n",
    "from nltk.stem import WordNetLemmatizer  # Importa el lematizador de palabras desde NLTK.\n",
    "from sklearn.feature_extraction.text import TfidfTransformer  # Importa el transformador TF-IDF de scikit-learn.\n",
    "from sklearn.feature_extraction.text import CountVectorizer  # Importa el vectorizador de recuento de palabras de scikit-learn.\n",
    "import numpy as np  # Importa la biblioteca NumPy para operaciones numéricas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 1. \n",
    "<br>\n",
    "Del archivo adjunto (Questions.csv), extraer las primeras 2,000 preguntas presentes en la columna \"Title\", \n",
    "y generar un nuevo archivo que SOLO contenga dichas preguntas.\n",
    "NOTA: El archivo es muy grande, por lo que deberás de procesarlo en tu equipo local (No usar colab), y \n",
    "generar con ayuda de un script el nuevo archivo, si llegas a tener problemas con el tipo de codificación\n",
    "y se generan caracteres raros, deberás resolverlo con la codificación adecuada de lectura"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Paso1 (): \n",
    "    \"\"\"\n",
    "    Este método sirve para extraer las primeras 2000 preguntas que se encuentran en la columna \"Title\", y también genera\n",
    "    un nuevo archivo que SOLO contenga dichas preguntas, es decir solo este la columna de Titulos con las preguntas. \n",
    "    Este método no recibe parametros y no devuelve nada.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Se lee el archivo Questions.csv con la funcion read_csv de la linreria de pandas y lo almacena en un dataframe\n",
    "    df = pd.read_csv(\"Questions.csv\", encoding='latin-1')\n",
    "    # Se extrae la columna Title del dataframe que creamos\n",
    "    df = df[\"Title\"]\n",
    "    # Se extraen las primeras 2000 preguntas \n",
    "    df = df.head(2000)\n",
    "    # Se crea un nuevo archivo csv con las preguntas extraidas\n",
    "    #df.to_csv(\"Preguntas.csv\", index=False)\n",
    "\n",
    "# Llamamos a la función Paso1 para ejecutar el proceso.\n",
    "Paso1()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso2. \n",
    "<br>\n",
    "Paso 2) \n",
    "Traducirlas las preguntas del nuevo archivo al idioma Español (Investiga cómo se realiza el proceso de \n",
    "traducción automático utilizando Google Sheets y Google Translate)\n",
    "Se deberá de agregar una nueva columna al archivo .csv llamada \"Textos_traducidos\" donde se incluirán las \n",
    "traducciones de los textos originales (Incluir este nuevo .csv en la entrega de tu tarea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 3.\n",
    "<br>\n",
    "Paso 3) \n",
    "Con este nuevo .csv de 2,000 textos traducidos al español, crear un dataset (de 2,000 textos) y aplicar \n",
    "OBLIGATORIAMENTE los siguientes preprocesamientos:<br>\n",
    "- Lematización de todas las palabras <br>\n",
    "- Filtrado de StopWords <br>\n",
    "- Pasar todo a minúsculas <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Paso3(df):\n",
    "    \"\"\"\n",
    "    Este método sirve para aplicar métodos de preprocesamiento a un DataFrame previamente traducido, que incluyen:\n",
    "    - Lematización de todas las palabras.\n",
    "    - Filtrado de Stop Words.\n",
    "    - Conversión de todas las palabras a minúsculas.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): El DataFrame que contiene preguntas traducidas del paso 2.\n",
    "\n",
    "    Returns:\n",
    "        list: Una lista que contiene todas las preguntas o textos después de realizar el preprocesamiento.\n",
    "    \"\"\"\n",
    "\n",
    "    # Se descargan los paquetes de nltk\n",
    "    nltk.download('punkt') #punkt es un tokenizador de oraciones \n",
    "    nltk.download('stopwords')#sacar las stopwords\n",
    "\n",
    "    # Lematiacion\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    # Se crea una lista vacia para almacenar las preguntas lematizadas\n",
    "    preguntas_lematizadas = []\n",
    "\n",
    "    # Se recorre la columna Textos_traducidos para lematizar cada pregunta\n",
    "    for pregunta in df:\n",
    "        # Se tokeniza cada pregunta\n",
    "        tokens = nltk.word_tokenize(pregunta)\n",
    "        #Pasar a minusculas todas ls palabras de las preguntas\n",
    "        tokens = [w.lower() for w in tokens]\n",
    "        #se crea una lista para guardar las palabras ya preprocesadas\n",
    "        palabras_lematizadas = []\n",
    "\n",
    "        # Se lematiza cada token \n",
    "        for token in tokens:\n",
    "            #Eliminar stopwords\n",
    "            if token not in nltk.corpus.stopwords.words('spanish'): #si el token no esta en stopwords se lematiza y se agrega a la lista de preguntas lematizadas\n",
    "                lematizado = lemmatizer.lemmatize(token)\n",
    "                # Se agrega el token lematizado a la lista de preguntas lematizadas\n",
    "                palabras_lematizadas.append(lematizado)\n",
    "\n",
    "        # Se agrega la pregunta lematizada a la lista de preguntas lematizadas\n",
    "        preguntas_lematizadas.append(\" \".join(palabras_lematizadas))\n",
    "\n",
    "    return preguntas_lematizadas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 4.\n",
    "<br>\n",
    "Paso 4) \n",
    "Después de haber limpiado el dataset anterior, generar el vector de idf correspondiente a TODOS los textos\n",
    "y mostrarlo en pantalla\n",
    "r en pantalla el top de las 10 palabras MAS relevantes, y el top de las 20 palabras MENOS \n",
    "relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Paso4 (preguntas_lematizadas):    \n",
    "    \"\"\"\n",
    "    Este método sirve para que después de haber realizado todo el preprocesamiento \n",
    "    del DataSet anterior, se genere un vector IDF para todas las preguntas y se \n",
    "    muestren en pantalla, en una tabla, las 10 palabras más relevantes en el texto \n",
    "    y las 20 menos relevantes.\n",
    "\n",
    "    Args:\n",
    "        preguntas_lematizadas (list): Una lista que contiene los textos o preguntas \n",
    "        después de haber aplicado el preprocesamiento.\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: Una tabla con las palabras más relevantes y sus valores IDF.\n",
    "    \"\"\"\n",
    "    #Se crea una instancia de CountVectorizer, que s eutiliza en el procesamiento\n",
    "    #para convertirlos en vectores númericos \n",
    "    CV = CountVectorizer()\n",
    "    VectorPalabras = CV.fit_transform(preguntas_lematizadas)\n",
    "    print(\"Forma del vector(Textos, Vocabulario) = \" + str(VectorPalabras.shape))\n",
    "    #print(VectorPalabras)\n",
    "\n",
    "    tfidf_transformer = TfidfTransformer(use_idf=True) #instancia de un transformador que le decimos que va a usar idf\n",
    "    tfidf_transformer.fit(VectorPalabras)#saca el idf (mide exclusividad) y tf (mide frecuencia)\n",
    "    # Insertamos los valores en un DataSet\n",
    "    df_idf = pd.DataFrame(tfidf_transformer.idf_, index = CV.get_feature_names_out(), columns = [\"Pesos-IDF\"])\n",
    "    # Ordenams los valores del de menor relevancia hasta el de mayor relevancia\n",
    "    df_idf.sort_values(by=['Pesos-IDF'], inplace = True)\n",
    "\n",
    "\n",
    "    return df_idf\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 5.\n",
    "<br>\n",
    "Paso 5) Regresar al Dataset original, y remover todas aquellas palabras que contengan un valor de idf menor\n",
    "al promedio de TODOS los idfs de la tabla obtenida\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def Paso5(lista, df_idf):\n",
    "    \"\"\"\n",
    "    Este método realiza el filtrado de palabras en una lista de textos (preguntas) basado en los valores IDF \n",
    "    calculados previamente. Se filtran las palabras cuyos valores IDF son menores que el promedio de valores IDF \n",
    "    en el DataFrame df_idf.\n",
    "\n",
    "    Args:\n",
    "        lista (list): Una lista de textos (preguntas) en la que se realizará el filtrado.\n",
    "        df_idf (pandas.DataFrame): Un DataFrame que contiene los valores IDF de las palabras.\n",
    "\n",
    "    Returns:\n",
    "        list: Una lista que contiene los textos (preguntas) filtrados, donde se han eliminado las palabras con valores \n",
    "        IDF por debajo del promedio.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calcular el promedio de los valores IDF en el DataFrame df_idf.\n",
    "    promedio = np.mean(df_idf['Pesos-IDF'])#usamos el metodo mean de la libreria numpy para sacar el promedio \n",
    "    print(promedio)#imprime el promedio \n",
    "\n",
    "    # Filtrar las palabras en el DataFrame df_idf y obtener una lista de palabras a eliminar.\n",
    "    col = [indice for indice, fila in df_idf.iterrows() if fila['Pesos-IDF'] < promedio]\n",
    "\n",
    "    # Función para eliminar palabras no permitidas en una celda de texto.\n",
    "    def eliminar_palabras_no_permitidas(celda, col):\n",
    "        \n",
    "        palabras = celda.split()  # Divide la celda de texto en una lista de palabras. La función split() separa las palabras por espacios en blanco.\n",
    "        palabras_filtradas = [palabra for palabra in palabras if palabra not in col]  # Crea una lista de palabras filtradas donde se eliminan las palabras que están en la lista 'col'.\n",
    "        return ' '.join(palabras_filtradas)  # Une las palabras filtradas en una cadena de texto separadas por un espacio en blanco y la devuelve.\n",
    "\n",
    "            \n",
    "\n",
    "    # Aplicar la función a todas las celdas de la lista de textos.\n",
    "    df_filtrado = [eliminar_palabras_no_permitidas(celda, col) for celda in lista]\n",
    "\n",
    "    return df_filtrado\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 6. \n",
    "<br>\n",
    "Paso 6)\n",
    "Generar nuevamente la tabla de idf a partir de valores de idf para los textos filtrados y mostrarla en pantalla\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paso 7. \n",
    "<br>\n",
    "Paso 7) Imprimir en pantalla el top de las 10 palabras MAS relevantes, y el top de las 20 palabras MENOS \n",
    "relevantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mich-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mich-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del vector(Textos, Vocabulario) = (2000, 3259)\n",
      "7.526163116687155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\mich-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mich-\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forma del vector(Textos, Vocabulario) = (2000, 2222)\n",
      "              Pesos-IDF\n",
      "error          7.908255\n",
      "equivalentes   7.908255\n",
      "equivalencia   7.908255\n",
      "episodio       7.908255\n",
      "envíe          7.908255\n",
      "envía          7.908255\n",
      "envuelvo       7.908255\n",
      "envoltura      7.908255\n",
      "escribir       7.908255\n",
      "únicos         7.908255\n",
      "         Pesos-IDF\n",
      "cómo      2.474533\n",
      "cuál      4.135494\n",
      "qué       4.650159\n",
      "hay       4.709582\n",
      "por       5.075042\n",
      "es        5.269198\n",
      "python    5.305565\n",
      "puedo     5.893352\n",
      "existe    6.116496\n",
      "cuáles    6.203507\n",
      "alguien   6.203507\n",
      "django    6.298817\n",
      "os        6.298817\n",
      "la        6.404178\n",
      "alguna    6.404178\n",
      "puede     6.521961\n",
      "dónde     6.521961\n",
      "cuándo    6.521961\n",
      "re        6.655492\n",
      "exe       6.655492\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Este código realiza una serie de operaciones en un archivo CSV llamado \"Preguntas.csv\". A continuación, se describen los pasos realizados:\n",
    "\n",
    "1. Se abre el archivo \"Preguntas.csv\" y se carga en un DataFrame llamado 'df'.\n",
    "\n",
    "2. Se lee la columna \"Textos_Traducidos\" del DataFrame 'df' y se almacena en la variable 'df'.\n",
    "\n",
    "3. Se llama a la función 'Paso3' con 'df' como argumento. 'Paso3' realiza el preprocesamiento de los textos, incluyendo la lematización y el filtrado de stopwords, y devuelve una lista de textos preprocesados. El resultado se almacena en la variable 'res'.\n",
    "\n",
    "4. Se llama a la función 'Paso4' con 'res' como argumento. 'Paso4' calcula los valores IDF para las palabras en los textos preprocesados y devuelve un DataFrame con estos valores. El resultado se almacena en la variable 'idfs'.\n",
    "\n",
    "5. Se llama a la función 'Paso5' con 'res' e 'idfs' como argumentos. 'Paso5' filtra palabras en los textos preprocesados basándose en los valores IDF y devuelve una lista de textos preprocesados filtrados. El resultado se almacena en 'df_filtrado'.\n",
    "\n",
    "6. Se llama nuevamente a 'Paso3' con 'df_filtrado' como argumento. Esto realiza el preprocesamiento de los textos filtrados y el resultado se almacena en 'res'.\n",
    "\n",
    "7. Se llama nuevamente a 'Paso4' con 'res' como argumento para recalcular los valores IDF basados en los textos preprocesados filtrados. El resultado se almacena en 'idfs'.\n",
    "\n",
    "8. Se seleccionan las 10 palabras con los valores IDF más altos de 'idfs' y se almacenan en 'Palabras_mas_relevantes'.\n",
    "\n",
    "9. Se seleccionan las 20 palabras con los valores IDF más bajos de 'idfs' y se almacenan en 'Palabras_menos_relevantes'.\n",
    "\n",
    "10. Se imprimen en la consola las 'Palabras_mas_relevantes' y las 'Palabras_menos_relevantes'.\n",
    "\n",
    "Este código realiza un análisis de relevancia de palabras en un conjunto de preguntas y muestra las palabras más y menos relevantes en función de sus valores IDF.\n",
    "\"\"\"\n",
    "\n",
    "# Se abre el archivo Preguntas.csv\n",
    "df = pd.read_csv(\"Preguntas.csv\")\n",
    "# Se lee la columna Textos_traducidos\n",
    "df = df[\"Textos_Traducidos\"]\n",
    "\n",
    "res = Paso3(df)\n",
    "idfs = Paso4(res)\n",
    "df_filtrado = Paso5(res,idfs)\n",
    "res = Paso3(df_filtrado)\n",
    "idfs = Paso4(res)\n",
    "\n",
    "Palabras_mas_relevante = idfs.tail(10)\n",
    "Palabras_menos_relevante = idfs.head(20)\n",
    "\n",
    "print(Palabras_mas_relevante)\n",
    "print(Palabras_menos_relevante)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
